\documentclass[a4paper]{article}

\def\nterm {Spring}
\def\nyear {2025}
\def\ncourse {Natural Language Processing}

\input{header}

\begin{document}
\maketitle

\tableofcontents

\section{Introduction}

Below is a list of topics that will be covered in this note. 
The topics include fundamental theoretical knowledge in 
language processing and language modeling in the first 
few sections, as well as frontier research results
in later sections.

\begin{enumerate}
  \item Language modeling fundamentals 
  \begin{enumerate}
    \item Representing words 
    \item Language modeling 
    \item Sequence modeling architectures
  \end{enumerate}

  \item Training and inference models
  \begin{enumerate}
    \item Decoding and Generation Algorithms
    \item In-context learning
    \item Pre-training
    \item Fine-tuning
    \item Reinforcement Learning
  \end{enumerate}

  \item Evaluation and Experimental Design
  \begin{enumerate}
    \item Evaluating Language Generators
    \item Experimental Design
    \item Human Annotation
    \item Debugging/Interpretation Techniques
  \end{enumerate}

  \item Advanced Algorithms and Architectures 
  \begin{enumerate}
    \item Advanced Pretraining, Post-Training, and Inference
    \item Retrieval and Retrieval-augmented Generation
    \item Long Sequence Models
    \item Distillation and Quantization
    \item Ensembling and Mixture of Experts
  \end{enumerate}

  \item NLP Applications and Society
  \begin{enumerate}
    \item Complex Reasoning Tasks
    \item Language Agents
    \item Multimodal NLP
    \item Multilingual NLP
    \item Bias and Fairness
  \end{enumerate}
\end{enumerate}

\section{Language Modeling Fundamentals}

\subsection{Neural Networks and Auto Differentiation}
This process calculates the gradient of the loss with 
respect to different parameters. Use computational graph 
and chain rule to compute back propogation efficiently, 
and implmented in PyTorch. With the calculated gradient, 
we can use Stochastic Gradient Descent (SGD) to update 
the model parameters. 
Other than PyTorch, TensorFlow and JAX are also popular 
neural network frameworks with different advantages.

\subsection{Language and Sequence Modeling} 

In language modeling, other than binary or multi-class 
classification, sometimes there are exponential/infinitely
many labels. This is called \textbf{structured prediction}. 
For example, assgining a part-of-speech tag for each word in the 
sentence. It is easy to note that in this case the number 
of labels grows exponentially with respect to the length 
of the sequence.
There are also distinctions between \textbf{uncondictioned 
prediction} and \textbf{conditioned prediction}, which predict 
the distribution of $P(X)$ and $P(Y \mid X)$, respectively.

A language model is a probability distribution over all 
sequences. It can be used to \textbf{score} sequences
and \textbf{generate} sequences. For example, in 
conditional generation, we condition on an input text and 
tries to continue it 
\[
\hat{x}_{t + 1: T} \sim p(X_{t+1: T} \mid x_{1 : t}).
\]
For another example, it can be used to translate between 
languages (machine translation). In all of these scenarios, 
the key is to model the following distribution:
\[
p(x_{t + 1} \mid x_1, x_2, \dots, x_n).
\]

\subsubsection{Bigram models and N-gram models} 
The assumption of bigram models is 
\[
p(X) \approx \prod_{t=1}^T p_\theta(x_{t} \mid x_{t - 1}).
\]
This is to say all tokens only depend on the token right 
before it. To train the bigram models, we simply count 
\[
p(x_t \mid x_{t - 1}) = \frac{\text{count} (x_{t-1}, x_t)}
{\sum_{x'} \text{count}(x_{t-1}, x')}.
\]
We can view $\theta_{i, j} = p(x_j \mid x_i)$ the 
parameters of the model. The reason behind the 
counting procedure is to produce a \textbf{maximum 
likelihood estimation}: 
\[
\max_{\theta} \sum_{x \in D_{\text{train}}} 
\log p_\theta(x).
\]
This essentially tries to match $p_\theta$ to $p_{\text{data}}$,
which can be more formally derived with KL divergence.

To evaluate the model, we can use the \textbf{log-likelihood}
and the \textbf{perplexity} of the model. The log-likelihood 
is simply defined as 
\[
  \mathrm{LL}(\mathcal{X}_{\text{test}}) 
  = \sum_{X \in \mathcal{X}_{\text{test}}} \log p(X).
\]
We can also use the per-word log-likelihood 
$\mathrm{WLL}(\mathcal{X}_{\text{test}})$,
which is defined as the log-likelihood divided by the 
length of the test data.
The perplexity of the model is 
$\exp \left( -\mathrm{WLL(\mathcal{X}_{\text{test}})} \right)$,
which can be used to simulate the number of tries it need to 
take before getting the correct prediction.

The bigram models or N-gram models in general have a lot of problems.
When $N$ gets large, the number of parameters will grow 
exponential with respect to $N$, but the occurrence of these 
parameters will be extremely rare.
Additionally, it cannot share strength among similar words, 
it cannot condition on context with intervening words, 
and it cannot handle long-distance dependencies. 
To try it out, use the 
\texttt{kenlm}\footnote{\texttt{
  https://github.com/kpu/kenlm
}} toolkit.

\subsubsection{Feedforward neural language model}
Use a neural netowrk to model 
\[
p(X) = \prod_{t = 1}^{T} p(x_t \mid x_{t-1}, \dots, x_{t-n+1})
\]
by miniming the loss 
\[
L = - \log p_\theta (x_t \mid x_{1 : t - 1}).
\]
This actually correspond to cross entropy loss.
An example structure of this is shown in Figure 
\ref{feedforward-nn-lm}.

\begin{figure}
  \centering
  \includegraphics[width=0.6\linewidth]{figs/forward-nn-lm.jpg}
  \caption{An illustration of feedforward neural language 
  model.}
  \label{feedforward-nn-lm}
\end{figure}

Note that since we can make the embeddings of word closed to 
each other if the words have similar meaning, it solves the 
problem of N-gram models that they cannot share strength 
between similar words. Neural models can also condition 
on context with intervening words, which is another advantage
compare to N-gram models.

\subsubsection{Practice tips for neural language models}
It is good to initialize weight matrices
using the \textbf{Xavier initialization} technique, which is 
implmented in function \texttt{nn.init.xavier\_uniform\_} 
in PyTorch. Weight initialization
is important in deep learning models, since it has a huge 
impact on the training results. 
For optimizer, instead of SGD, the most 
standard optimization method is \textbf{Adam}, 
which is also implmented in PyTorch. 
For the choice of learning rate, 
we also have consine learning rate schedule and warmup 
techniques.

Check out paper to learn more about these methods.

\end{document}